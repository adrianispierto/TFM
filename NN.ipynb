{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa21a16-5816-42bf-923e-a122e7b6353b",
   "metadata": {},
   "source": [
    "# Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08150b-475e-46f6-852a-3de1781e63de",
   "metadata": {},
   "source": [
    "Este c√≥digo lee las im√°genes estra√≠das de los c√≥digos anteriores, los transforma y realiza el preproceso de las variables. A continuaci√≥n, define los distintos modelos estudiados y los eval√∫a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe5224-fd63-42b8-8879-5449d8593c7d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285d0d4-6e6a-4533-a7d0-4d1ec9fbc7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3892c9d-be09-4a83-95c7-199a76945ab8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Paso 0: Definici√≥n de par√°metros y funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79379ec8-06ed-42d7-b86b-b9f4387c34e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estructura de directorios\n",
    "DATA_DIR = Path.cwd().resolve() / \"data\"\n",
    "SENTINEL_DATA_DIR = DATA_DIR / \"sentinel\"\n",
    "LANDSAT_DATA_DIR = DATA_DIR / \"landsat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85153ca-139f-43ff-bf78-7c8b4c5ebb3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tama√±o de las im√°genes a las que se han de ajustar los datos extra√≠dos, ya sea truncando o a√±adiendo p√≠xeles dummies.\n",
    "# Las im√°genes extra√≠das var√≠an su n√∫mero de filas y columnas a√∫n siendo de la misma resoluci√≥n\n",
    "SHAPE = (3, 42, 42) # Corresponde a un detalle de 420*420 m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f93411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decodificaciones para los datos de Landsat\n",
    "WATER_BIT_FILTER = np.vectorize(lambda num: int(format(num,'#018b')[-8]))\n",
    "CLEAR_BIT_FILTER = np.vectorize(lambda num: int(format(num,'#018b')[-7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db2e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pxl20to10(arr20, shape):\n",
    "    '''\n",
    "    Fragmenta una matriz con resoluci√≥n de 20 m/pxl, a una con 10 m/pxl.\n",
    "    Para ello divide cada p√≠xel en 4 de igual valor.\n",
    "    \n",
    "    Retorna el vector con las nuevas dimensiones dadas por shape.\n",
    "    '''\n",
    "    \n",
    "    out = np.zeros((1, shape[1], shape[2]), dtype=\"uint8\")\n",
    "    len_row = arr20.shape[2]*2\n",
    "    \n",
    "    if len_row >= shape[2]:\n",
    "        for i in range(arr20.shape[1]):\n",
    "            row = np.repeat(arr20[0, i, :], 2)\n",
    "            if 2*i <=  shape[1]-1: \n",
    "                out[0, 2 * i, :] = row[0 : shape[2]]\n",
    "            if 2*i + 1 <=  shape[1]-1: \n",
    "                out[0, 2 * i + 1, :] = row[0 : shape[2]]\n",
    "                \n",
    "        return out\n",
    "    \n",
    "    else:\n",
    "        for i in range(arr20.shape[1]):\n",
    "            row = np.repeat(arr20[0, i, :], 2)\n",
    "            if 2*i <=  shape[1]-1: \n",
    "                out[0, 2 * i, :] = row\n",
    "            if 2*i + 1 <=  shape[1]-1: \n",
    "                out[0, 2 * i + 1, :] = row\n",
    "                \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea21892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pxl30to10(arr30):\n",
    "    '''\n",
    "    Fragmenta una matriz con resoluci√≥n de 30 m por p√≠xel, a una con 10 m/pxl.\n",
    "    Para ello divide cada p√≠xel en nueve de igual valor\n",
    "    '''\n",
    "    \n",
    "    out = np.zeros((3,arr30.shape[1]*3, arr30.shape[2]*3), dtype=\"uint8\")\n",
    "    \n",
    "    for i in range(arr30.shape[1]):  \n",
    "        row = np.repeat(arr30[0, i, :], 3)\n",
    "        \n",
    "        out[0, 3 * i, :] = row\n",
    "        out[0, 3 * i + 1, :] = row\n",
    "        out[0, 3 * i + 2, :] = row\n",
    "        \n",
    "        row1 = np.repeat(arr30[1, i, :], 3)\n",
    "        \n",
    "        out[1, 3 * i, :] = row1\n",
    "        out[1, 3 * i + 1, :] = row1\n",
    "        out[1, 3 * i + 2, :] = row1\n",
    "        \n",
    "        row2 = np.repeat(arr30[2, i, :], 3)\n",
    "        \n",
    "        out[2, 3 * i, :] = row2\n",
    "        out[2, 3 * i + 1, :] = row2\n",
    "        out[2, 3 * i + 2, :] = row2\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10886ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cla_a_estimation(img_SCL, img_B03, img_B04, img_B05, log=0, sat='sentinel'):\n",
    "    '''\n",
    "    Estimaci√≥n de los niveles de clorofila-a a partir de las bandas 3, 4 y 5 del Sentinel-2 MSI.\n",
    "    La expresi√≥n utilizada es la calculada por Zhan et al.:\n",
    "        124.94 * (b03_img + b05_img) / (b03_img + b04_img) - 115.35\n",
    "    \n",
    "    Para los datos de Landsat se estima el NDCI a partir de la expresi√≥n (Gabila Buma y Lee): \n",
    "        ùëÅùê∑ùê∂ùêº = 0.6396 ‚àó (ùëÅùêºùëÖ ‚àí ùëÖùëíùëë) /(ùëÅùêºùëÖ + ùëÖùëíùëë) + 0.4096\n",
    "        \n",
    "    Devuelve una lista con el valor medio estimado, su desviaci√≥n t√≠pica y el log.\n",
    "    '''\n",
    "    if sat == 'sentinel':\n",
    "        # Comprobaci√≥n de que todas las im√°genes tienen las mismas dimensiones\n",
    "        shape_SCL = img_SCL.shape # (1,N,M). Resolucion de 20 m\n",
    "        shape_B03 = img_B03.shape # (1,N,M)\n",
    "        shape_B04 = img_B04.shape # (1,N,M)\n",
    "        shape_B05 = img_B05.shape # La resoluci√≥n de la banda 5 es de 20 m. Es necesario redimensionar la matriz\n",
    "\n",
    "        if  (shape_B03[1:-1] != shape_B04[1:-1]):\n",
    "            log = \"Dimensiones no coincidentes entre resoluciones iguales (10m)\"\n",
    "            raise Exception(\"Error de tama√±o en 10m\")\n",
    "\n",
    "        # Redimensionado. Convierte la matriz (1,n,m) en (1, shape[1], shape[2]) truncando si es necesario la matriz objetivo de (1,2n,2m)\n",
    "        new_img_B05 = pxl20to10(img_B05, shape_B04)\n",
    "        new_img_SCL = pxl20to10(img_SCL, shape_B04)\n",
    "        \n",
    "        if  (new_img_B05.shape[1:-1] != shape_B03[1:-1]) or (new_img_SCL.shape[1:-1] != shape_B03[1:-1]):\n",
    "            log = \"Dimensiones no coincidentes tras redimensionado\"\n",
    "            raise Exception(\"Error de tama√±o en 20m tras redimensionado\")\n",
    "\n",
    "        # Calculo de la estimaci√≥n de clorofila-a (en bruto).\n",
    "        CHLa = np.zeros(shape_B04)\n",
    "        for i in range(shape_B04[1]):\n",
    "            for j in range(shape_B04[2]):\n",
    "                \n",
    "                # Se guardan en variables de numero flotante para evitar el overflowing del dato al ser en origen un entero.\n",
    "                R705 = float(new_img_B05[0,i,j])\n",
    "                R665 = float(img_B04[0,i,j])\n",
    "                \n",
    "                if R705 + R665 != 0:\n",
    "                    CHLa[0,i,j] = 0.8489 * (R705 - R665) / (R705 + R665) - 0.1894\n",
    "                else:\n",
    "                    CHLa[0,i,j] = 0\n",
    "                    \n",
    "        # Filtrado seg√∫n los p√≠xeles de agua\n",
    "        CHLa_MSK = CHLa[new_img_SCL == 4]\n",
    "        if len(CHLa_MSK) == 0:\n",
    "            mean_cla_a_value = 0\n",
    "            std_cla_a_value = 0\n",
    "        else:\n",
    "            mean_cla_a_value = np.mean(CHLa_MSK)\n",
    "            std_cla_a_value = np.std(CHLa_MSK)\n",
    "\n",
    "        return [mean_cla_a_value, std_cla_a_value, log]\n",
    "    \n",
    "    else: # Landsat\n",
    "        # imgB03 -> Rojo. 660nm\n",
    "        # imgB04 -> NIR. 830nm\n",
    "        # imgB05 -> None\n",
    "        # Comprobaci√≥n de que todas las im√°genes tienen las mismas dimensiones. Todas tienen 30 m de resoluci√≥n\n",
    "        shape_SCL = img_SCL.shape # (N,M)\n",
    "        shape_Red = img_B03.shape # (N,M)\n",
    "        shape_NIR = img_B04.shape # (N,M)\n",
    "\n",
    "        if (shape_SCL!=shape_Red) and (shape_SCL!=shape_NIR) and (shape_Red!=shape_NIR):\n",
    "            log = \"Dimensiones no coincidentes entre resoluciones iguales (30m)\"\n",
    "            raise Exception(\"Error de tama√±o en 30m\")\n",
    "        \n",
    "        # Calculo de la estimaci√≥n de clorofila-a (en bruto). Indice NDCI.\n",
    "        CHLa = np.zeros(shape_Red)\n",
    "        for i in range(shape_Red[0]):\n",
    "            for j in range(shape_Red[1]):\n",
    "                \n",
    "                # Se guardan en variables de numero flotante para evitar el overflowing del dato al ser en origen un entero.\n",
    "                R865 = float(img_B04[i,j])\n",
    "                R654 = float(img_B03[i,j])\n",
    "                if R865 + R654 != 0:\n",
    "                    CHLa[i,j] = 0.6396 * (R865 - R654) / (R865 + R654) - 0.4096\n",
    "                else:\n",
    "                    CHLa[i,j] = 0\n",
    "        \n",
    "        # Filtrado seg√∫n los p√≠xeles de agua\n",
    "        CHLa_MSK = CHLa[img_SCL == 1]\n",
    "        if len(CHLa_MSK) == 0:\n",
    "            mean_cla_a_value = 0\n",
    "            std_cla_a_value = 0\n",
    "        else:\n",
    "            mean_cla_a_value = np.mean(CHLa_MSK)\n",
    "            std_cla_a_value = np.std(CHLa_MSK)\n",
    "                \n",
    "        return [mean_cla_a_value, std_cla_a_value, log]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64045674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visible_water_surface_estimation(img_SCL, resolution=20, sat='sentinel'):\n",
    "    '''\n",
    "    Calcula la superficie visible de agua en m^2. Puede ser √∫til para detectar peque√±as masas de agua cuyo din√°mica de \n",
    "    propagaci√≥n de HAB difiere de las masas m√°s grandes.\n",
    "    \n",
    "    Devuelve un entero con la superficie de agua visible en el recorte.\n",
    "    '''\n",
    "    if sat=='sentinel':\n",
    "        water_pixels = len(img_SCL[img_SCL == 4])\n",
    "    else:\n",
    "        water_pixels = len(img_SCL[img_SCL == 1])\n",
    "    \n",
    "    VWS = water_pixels*resolution**2\n",
    "    \n",
    "    return VWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58bbb3-8b87-4f67-9d69-171f549f055d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uniformar_imagen(img_TCI, sat='sentinel'):\n",
    "    '''\n",
    "    Redimensiona la imagen TCI.\n",
    "    \n",
    "    Devuelve la misma im√°gen truncando o a√±adiendo filas/columnas nulas seg√∫n las dimensiones especificadas\n",
    "    con el par√°metro SHAPE.\n",
    "    '''\n",
    "    if sat == 'landsat':\n",
    "        # Primero se dividen los p√≠xeles de 30m en 9 p√≠xeles de 10m.\n",
    "        img_TCI = pxl30to10(img_TCI)\n",
    "        shape_i = img_TCI.shape\n",
    "    else:\n",
    "        shape_i = img_TCI.shape\n",
    "        \n",
    "    new_img_TCI = np.zeros(SHAPE, dtype=\"uint8\")\n",
    "    \n",
    "    num_filas = shape_i[1] - SHAPE[1]\n",
    "    num_columnas = shape_i[2] - SHAPE[2]\n",
    "    \n",
    "    if num_columnas <= 0 and num_filas <= 0: # se a√±aden columnas y filas (ceros)\n",
    "        new_img_TCI[:, 0:shape_i[1], 0:shape_i[2]] = img_TCI[:,:,:]\n",
    "\n",
    "    elif num_columnas >= 0 and num_filas >= 0: # se truncan las filas y columnas sobrantes\n",
    "        new_img_TCI = img_TCI[:, 0:SHAPE[1], 0:SHAPE[2]]\n",
    "\n",
    "    elif num_columnas <= 0 and num_filas >= 0: # se truncan las filas y se a√±aden columnas\n",
    "        new_img_TCI[:, :, 0:shape_i[2]]  = img_TCI[:, 0:SHAPE[1], :]\n",
    "    \n",
    "    elif num_columnas >= 0 and num_filas <= 0: # se a√±aden las filas y se truncan columnas\n",
    "        new_img_TCI[:, 0:shape_i[1], :]  = img_TCI[:, :, 0:SHAPE[2]]\n",
    "\n",
    "    return new_img_TCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a560b9d-a655-442c-9d7b-c81699fa6374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconversion_target(predictions, labels, subset):\n",
    "    '''\n",
    "    Deshace la transformaci√≥n lineal sobre la clase y calcula la precisi√≥n y las errores medios absolutos y cuadr√°ticos\n",
    "    promediados por la regi√≥n donde se realiz√≥ la medici√≥n.\n",
    "    \n",
    "    Devuelve la precisi√≥n, el mse promediado por region, el mae promediado por region y una tupla con los rmse para cada region\n",
    "    '''\n",
    "    rounded_result = []\n",
    "    acc_result = []\n",
    "    err = []\n",
    "    rmse = {'w':[], 'mw': [], 'ne':[], 's': []}\n",
    "    rmae = {'w':[], 'mw': [], 'ne':[], 's': []}\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        rounded_result.append((predictions[i] * 4 + 1).round())\n",
    "        err.append(abs(rounded_result[-1]-labels.iloc[i]*4-1))\n",
    "\n",
    "        if subset.region.iloc[i] == 'west':\n",
    "            rmse['w'].append(err[-1]**2)\n",
    "            rmae['w'].append(err[-1])\n",
    "        elif subset.region.iloc[i] == 'south':\n",
    "            rmse['s'].append(err[-1]**2)\n",
    "            rmae['s'].append(err[-1])\n",
    "        elif subset.region.iloc[i] == 'northeast':\n",
    "            rmse['ne'].append(err[-1]**2)\n",
    "            rmae['ne'].append(err[-1])\n",
    "        elif subset.region.iloc[i] == 'midwest':\n",
    "            rmse['mw'].append(err[-1]**2)\n",
    "            rmae['mw'].append(err[-1])\n",
    "\n",
    "        if rounded_result[-1] == labels.iloc[i]*4+1:\n",
    "            acc_result.append(1)\n",
    "        else:\n",
    "            acc_result.append(0)\n",
    "            \n",
    "    acc = acc_result.count(1)/len(acc_result)\n",
    "    rmae = (np.mean(rmae['w']) + np.mean(rmae['mw'])  + np.mean(rmae['ne'])  + np.mean(rmae['s'])) / 4\n",
    "    rmse_val = (np.mean(rmse['w'])**0.5 + np.mean(rmse['mw'])**0.5  + np.mean(rmse['ne'])**0.5  + np.mean(rmse['s'])**0.5) / 4\n",
    "    \n",
    "    return acc, rmse_val, rmae, (np.mean(rmse['w'])**0.5,np.mean(rmse['mw'])**0.5,np.mean(rmse['ne'])**0.5,np.mean(rmse['s'])**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c0101-63e0-438b-ad87-3843a0a43065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_clouds(scl, sat='sentinel'):\n",
    "    '''\n",
    "    Calcula el porcentaje de nubosidad presente en la imagen diferenciando el origen de los datos.\n",
    "    \n",
    "    Devuelve un flotante con el porcentaje de nubosidad\n",
    "    '''\n",
    "    \n",
    "    if sat=='sentinel':\n",
    "        pxl_nubes = np.size( scl[ (scl == 3) | (scl == 8) | (scl == 9) | (scl == 10)] )\n",
    "        total = np.size(scl)\n",
    "\n",
    "        porc_nubosidad = pxl_nubes / total * 100\n",
    "    else:\n",
    "        cloud_image_array = CLEAR_BIT_FILTER(scl)\n",
    "        pxl_nubes = np.size(cloud_image_array[cloud_image_array==1])\n",
    "        total = np.size(scl)\n",
    "        \n",
    "        porc_nubosidad = pxl_nubes / total * 100\n",
    "                \n",
    "    return porc_nubosidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebaa3f-65da-4270-93a7-b56d3d18cc28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mae_percent_error(pred, test):\n",
    "    '''\n",
    "    Calcula el porcentaje de mae tras deshacer la transformaci√≥n lineal sobre la clase.\n",
    "    \n",
    "    Devuelve un flotante con el valor medio del error absoluto en t√©rminos porcentuales.\n",
    "    '''\n",
    "    \n",
    "    maepe = []\n",
    "    for i in range(len(predictions)):\n",
    "        predi = pred[i] * 4 + 1\n",
    "        testi = test.iloc[i] * 4 + 1\n",
    "\n",
    "        maepe.append(100*abs(predi-testi)/testi)\n",
    "        \n",
    "    return np.mean(maepe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bfc186-0a11-45eb-b551-ab3e502e2581",
   "metadata": {},
   "source": [
    "## Paso 1: Preparar los datos de entrenamiento y prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087c8d9-8bef-4d2f-8322-449e1ec53017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# carga de los metadatos\n",
    "metadata = pd.read_csv(DATA_DIR / \"metadata.csv\")\n",
    "\n",
    "# campo de tipo de dato del campo date a datetime\n",
    "metadata.date = pd.to_datetime(metadata.date)\n",
    "\n",
    "# estaci√≥n en el momento de la medida\n",
    "metadata[\"season\"] = (\n",
    "    metadata.date.dt.month.replace([12, 1, 2], \"invierno\")\n",
    "    .replace([3, 4, 5], \"primavera\")\n",
    "    .replace([6, 7, 8], \"verano\")\n",
    "    .replace([9, 10, 11], \"oto√±o\")\n",
    ")\n",
    "\n",
    "# carga de las etiquetas\n",
    "labels = pd.read_csv(DATA_DIR / \"train_labels.csv\")\n",
    "\n",
    "metadata = metadata[metadata.usable=='ok']\n",
    "\n",
    "# join entre los metadatos y las etiquetas\n",
    "labels_and_metadata = labels.merge(\n",
    "    metadata, how=\"left\", left_on=\"uid\", right_on=\"uid\", validate=\"1:1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2afeb6-f34d-4a44-8994-8ac6ada0da1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformaci√≥n y carga de los datos extra√≠dos de Sentinel-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c3ef7-46e3-4386-a1a2-6d2a2184ded8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(SENTINEL_DATA_DIR / f\"selected_items.txt\", \"rb\") as f:\n",
    "    selected_items = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541c402-6baf-4713-b4e2-5780f1633a7a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_sentinel = pd.DataFrame(columns=[\"uid\", \"TCI\", \"cla_mean\", \"cla_std\", \"water_size\", \"T\", \"QI\"])\n",
    "log = {}\n",
    "for row in tqdm(metadata.itertuples(), total=len(metadata)):\n",
    "    try:\n",
    "        log[row.uid] = 0\n",
    "        # imagen a color verdadero (TCI). 10m de resoluci√≥n\n",
    "        with open(SENTINEL_DATA_DIR / f\"visual/{row.uid}.npy\", \"rb\") as f:\n",
    "            img_TCI = np.load(f)\n",
    "        # banda 3 (560 nm). 10m de resoluci√≥n\n",
    "        with open(SENTINEL_DATA_DIR / f\"b03/{row.uid}.npy\", \"rb\") as f:\n",
    "            img_B03 = np.load(f)\n",
    "        # banda 4 (665 nm). 10m de resoluci√≥n\n",
    "        with open(SENTINEL_DATA_DIR / f\"b04/{row.uid}.npy\", \"rb\") as f:\n",
    "            img_B04 = np.load(f)\n",
    "        # banda 5 (705 nm). 20m de resoluci√≥n\n",
    "        with open(SENTINEL_DATA_DIR / f\"b05/{row.uid}.npy\", \"rb\") as f:\n",
    "            img_B05 = np.load(f)\n",
    "        # Scene Classification Map. 20 m de resoluci√≥n\n",
    "        with open(SENTINEL_DATA_DIR / f\"scl/{row.uid}.npy\", \"rb\") as f:\n",
    "            img_SCL = np.load(f)\n",
    "\n",
    "        # Superficie de agua visible en la imagen\n",
    "        visible_water_surface = visible_water_surface_estimation(img_SCL)\n",
    "\n",
    "        # Calculo estimado de la concentraci√≥n de clorofila a (a partir del indice NDCI)\n",
    "        if visible_water_surface > 0:\n",
    "            [cla_mean, cla_std, log[row.uid]] = cla_a_estimation(img_SCL, img_B03, img_B04, img_B05, log[row.uid])\n",
    "        else:\n",
    "            cla_mean = 0\n",
    "            cla_std = 0\n",
    "            log[row.uid] = 'No se detectaron p√≠xeles de agua'\n",
    "\n",
    "        # Redimensionamiento de la imagen a color verdadero para uniformar las dimensiones de todos los uid's\n",
    "        img_TCI = uniformar_imagen(img_TCI)\n",
    "        if img_TCI.shape != SHAPE:\n",
    "            log[row.uid] = 'No uniformado.'\n",
    "            print('err')\n",
    "            raise Exception('No uniformado.')\n",
    "            \n",
    "        # Construcci√≥n del √≠ndice de calidad de la imagen\n",
    "        score_time_delta = 100 * int(selected_items[row.uid][\"time_diff\"].split(\" days 00:00:00\")[0]) / 15\n",
    "        score_nubosidad = 100 - check_clouds(img_SCL)\n",
    "        score_resolucion = 100\n",
    "        if visible_water_surface > 0:\n",
    "            score_agua_detectada = 100\n",
    "        else:\n",
    "            score_agua_detectada = 0\n",
    "\n",
    "        final_score = score_time_delta * 0.4 + score_nubosidad * 0.3 + score_resolucion * 0.1 + score_agua_detectada * 0.2\n",
    "        \n",
    "        new_row = pd.DataFrame({\n",
    "            \"uid\": row.uid,\n",
    "            \"TCI\": [img_TCI],\n",
    "            \"cla_mean\": cla_mean,\n",
    "            \"cla_std\": cla_std,\n",
    "            \"water_size\":visible_water_surface,\n",
    "            \"T\":-1, # Se completar√° posteriormente con los datos de HHRR NOAH\n",
    "            \"QI\": final_score\n",
    "        })\n",
    "        \n",
    "        data_sentinel = pd.concat([data_sentinel,new_row], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        log[row.uid] = str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3393dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(SENTINEL_DATA_DIR / \"TL_SENTINEL_log.txt\", \"w\") as f:\n",
    "    json.dump(log,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ded32-5902-43f3-92aa-6e87c4b7d0e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_sentinel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe549f-086f-4f8c-9c2f-88dc0faaa03f",
   "metadata": {},
   "source": [
    "Todas las im√°genes deben tener las mismas dimensiones. Pese a emplear la misma resoluci√≥n, al extraer las im√°genes mediante la API de Microsoft Planetary Computer, se ajustan las dimensiones f√≠sicas a un n√∫mero entero de p√≠xeles produci√©ndose cierta variabilidad en la representaci√≥n entre distintas im√°genes seg√∫n la aproximaci√≥n que se realiza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a9268-b1cf-4487-adf3-8545f65f1e04",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for ind,row in data_sentinel.iterrows():\n",
    "    if row.TCI.shape != SHAPE:\n",
    "        i+=1\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358338f-b366-45f0-9652-8057a97fa33d",
   "metadata": {},
   "source": [
    "Todo ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc28e9-d884-4297-9e84-e6de9b7426d7",
   "metadata": {},
   "source": [
    "A continuaci√≥n se a√±ade la informaci√≥n de temperatura obtenida de HRRR NOAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4c617-90c6-45d6-b679-c7f61642b6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(SENTINEL_DATA_DIR / f\"temperatures.txt\", \"rb\") as f:\n",
    "    temperatures = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a68ba6-c8dc-4067-b632-826a773cf4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_T = pd.DataFrame(pd.Series(temperatures), columns=['T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7721c3-cf53-4a0a-abe6-1ff0d258ee1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_sentinel = data_sentinel.merge(df_T, how='left', left_on='uid', right_index=True).drop('T_x', axis=1).rename(columns={\"T_y\":\"T\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1f937-00c7-4c94-98cd-d6c3648d512c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_sentinel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4912e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformaci√≥n y carga de los datos extra√≠dos de Landsat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712ed03-165c-4271-9788-ac2cf69624fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(LANDSAT_DATA_DIR / f\"selected_items.txt\", \"rb\") as f:\n",
    "    selected_items = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfb14e-4973-4838-acd7-9c1c97cb40ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(LANDSAT_DATA_DIR / f\"selected_items7.txt\", \"rb\") as f:\n",
    "    selected_items7 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16ccb0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_landsat = pd.DataFrame(columns=[\"uid\", \"TCI\", \"cla_mean\", \"cla_std\", \"water_size\", \"T\", \"QI\"])\n",
    "log = {}\n",
    "for row in tqdm(metadata.itertuples(), total=len(metadata)):\n",
    "    try:\n",
    "        log[row.uid] = 0\n",
    "        # imagen a color verdadero (TCI). 30m de resoluci√≥n [Red, Green, Blue]\n",
    "        with open(LANDSAT_DATA_DIR / f\"visual/{row.uid}.npy\", \"rb\") as f:\n",
    "            img_TCI = np.load(f)\n",
    "        # Banda lwir. Permite estimar la temperatura mediante la siguiente transformaci√≥n lineal: temp * 0.00341802 + 149.\n",
    "        # 30m de resoluci√≥n\n",
    "        with open(LANDSAT_DATA_DIR / f\"temperature/{row.uid}.npy\", \"rb\") as f:\n",
    "            img_T = np.load(f)\n",
    "        # Banda de Quality Assesment de los p√≠xeles. Incluye informaci√≥n de la nubosidad as√≠ de como p√≠xeles de agua. \n",
    "        # 30m de resoluci√≥n\n",
    "        with open(LANDSAT_DATA_DIR / f\"cloud/{row.uid}.npy\", \"rb\") as f:\n",
    "            img_QA = np.load(f)\n",
    "        # banda NIR (Near Infrarred). Se usa en la estimaci√≥n de cla-a. 30m de resoluci√≥n\n",
    "        with open(LANDSAT_DATA_DIR / f\"nir/{row.uid}.npy\", \"rb\") as f:\n",
    "            img_NIR = np.load(f)\n",
    "\n",
    "        # Superficie de agua visible en la imagen\n",
    "        water_img = WATER_BIT_FILTER(img_QA)\n",
    "        visible_water_surface = visible_water_surface_estimation(water_img, 30, 'landsat')\n",
    "\n",
    "        # Calculo estimado de la concentraci√≥n de clorofila a\n",
    "        if visible_water_surface > 0:\n",
    "            [cla_mean, cla_std, log[row.uid]] = cla_a_estimation(water_img[0], img_TCI[0], img_NIR[0], None, log[row.uid], 'landsat')\n",
    "        else:\n",
    "            cla_mean = 0\n",
    "            cla_std = 0\n",
    "            log[row.uid] = 'No se detectaron p√≠xeles de agua'\n",
    "\n",
    "        # Estimaci√≥n la temperatura media a partir de la se√±al lwir de LANDSAT. Aplicando la transformaci√≥n lineal dada por las especificaciones\n",
    "        # del producto: temp * 0.00341802 + 149\n",
    "        temperature = np.mean(img_T) * 0.00341802 + 149\n",
    "        \n",
    "         # Construcci√≥n del √≠ndice de calidad de la imagen\n",
    "        if row.uid in selected_items:\n",
    "            score_time_delta = 100 * int(selected_items[row.uid][\"time_diff\"].split(\" days 00:00:00\")[0]) / 15\n",
    "        else:\n",
    "            score_time_delta = 100 * int(selected_items7[row.uid][\"time_diff\"].split(\" days 00:00:00\")[0]) / 15\n",
    "        score_nubosidad = 100 - check_clouds(img_QA, sat='landsat')\n",
    "        score_resolucion = 0\n",
    "        if visible_water_surface > 0:\n",
    "            score_agua_detectada = 100\n",
    "        else:\n",
    "            score_agua_detectada = 0\n",
    "\n",
    "        final_score = score_time_delta * 0.4 + score_nubosidad * 0.3 + score_resolucion * 0.1 + score_agua_detectada * 0.2\n",
    "\n",
    "        # Redimensionamiento de la imagen a color verdadero para uniformar las dimensiones de todos los uid's\n",
    "        img_TCI = uniformar_imagen(img_TCI, 'landsat')\n",
    "\n",
    "        new_row = pd.DataFrame({\n",
    "            \"uid\": row.uid,\n",
    "            \"TCI\": [img_TCI],\n",
    "            \"cla_mean\": cla_mean,\n",
    "            \"cla_std\": cla_std,\n",
    "            \"water_size\": visible_water_surface,\n",
    "            \"T\": temperature,\n",
    "            \"QI\": final_score\n",
    "        })\n",
    "\n",
    "        data_landsat = pd.concat([data_landsat,new_row], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        log[row.uid] = str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc563f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(LANDSAT_DATA_DIR / \"TL_LANDSAT_log.txt\", \"w\") as f:\n",
    "    json.dump(log,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea85b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_landsat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de70d06",
   "metadata": {},
   "source": [
    "Todas las im√°genes deben tener las mismas dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25ed6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for ind,row in data_landsat.iterrows():\n",
    "    if row.TCI.shape != SHAPE:\n",
    "        i+=1\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8dd5ef-4767-47a6-92aa-df380c178d6f",
   "metadata": {},
   "source": [
    "Ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80879c20-caaa-4b15-89b1-b37316364954",
   "metadata": {},
   "source": [
    "Concatenaci√≥n de los datos extra√≠dos de sentinel y landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f7052-6c56-45e2-9dbe-a7ce6f0db03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e1120-735a-4450-8633-a2732014e109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data = pd.concat([data_landsat,df_data_sentinel], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ecfa4-fffc-4875-8dca-0cf235652b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_qi25 = df_data[df_data.QI>=25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791c9eb-75cf-4443-af52-1041169c4c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_qi50 = df_data[df_data.QI>=50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3274d56-2d68-48ae-9997-4b3efcbdaf6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Paso 2: Preparaci√≥n de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287958f5-8467-4676-9a76-9721cf793d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = labels_and_metadata.merge(\n",
    "    df_data, how=\"inner\", left_on=\"uid\", right_on=\"uid\", validate=\"1:1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff745d2-6236-4279-bb8c-e4716be7c50e",
   "metadata": {},
   "source": [
    "Split del dataset (estratificado por la clase objetivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b540a1-116d-42f5-a78b-6bad35cf46d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_subset, test_subset = train_test_split(df_final, test_size=0.2, random_state=10, stratify=df_final.severity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a542ac27-a1c0-4b0b-8aff-566afdb01723",
   "metadata": {},
   "source": [
    "Onehot encoding del campo season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a89d5-0bc1-4790-930f-2e4c56da3409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "season_map = {'invierno': 0, 'primavera':1, 'verano': 2, 'oto√±o': 3} \n",
    "train_subset[['invierno', 'primavera', \"verano\", \"oto√±o\"]] = to_categorical(train_subset.season.map(season_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c0565-1b7c-4d33-a889-9e0441be82ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_subset[['invierno', 'primavera', \"verano\", \"oto√±o\"]] = to_categorical(test_subset.season.map(season_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93732944-2ea0-427c-9926-90aeccb92f0f",
   "metadata": {},
   "source": [
    "Onehot encoding del campo region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01cc71-ed39-4880-9f0a-ed15e59f0908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_map = {'south': 0, 'west':1, 'midwest': 2, 'northeast': 3} \n",
    "train_subset[['south', 'west', \"midwest\", \"northeast\"]] = to_categorical(train_subset.region.map(region_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a76ef-6d48-4740-bc80-3eade9701fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_subset[['south', 'west', \"midwest\", \"northeast\"]] = to_categorical(test_subset.region.map(region_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa924d-761f-4c2a-99db-294d78c75b04",
   "metadata": {},
   "source": [
    "Separaci√≥n de los inputs en imagen y caracter√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b47f65-d1e8-403f-8505-928a7335d2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = train_subset[[\"cla_mean\", \"cla_std\", \"water_size\", \"T\", \"invierno\", 'primavera', \"verano\", \"oto√±o\", 'south', 'west', \"midwest\", \"northeast\"]]\n",
    "test_features = test_subset[[\"cla_mean\", \"cla_std\", \"water_size\", \"T\", \"invierno\", 'primavera', \"verano\", \"oto√±o\", 'south', 'west', \"midwest\", \"northeast\"]]\n",
    "\n",
    "train_img = train_subset[[\"TCI\"]]\n",
    "test_img = test_subset[[\"TCI\"]]\n",
    "\n",
    "# El m√≠nimo nivel de severidad es 1, se reducen todos uno para empezar desde 0 y aplicar la activaci√≥n sigmoide en la capa final\n",
    "train_labels = train_subset.severity-1\n",
    "test_labels = test_subset.severity-1                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73a1ad-0b21-41a4-b2ce-89a44a4a9e65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len_train = len(train_subset)\n",
    "len_test = len(test_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f149399-f443-4733-8e93-1fc40d57e97a",
   "metadata": {},
   "source": [
    "Reescalado de las im√°genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d9862-4960-45e5-92b4-69a72f6f4593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se elimina la primera dimensi√≥n de las im√°genes empleada al empaquetar la imagen durante la carga dentro del df\n",
    "train_img2 = train_img.TCI.apply(lambda item:  np.array([item[0], item[1], item[2]]))\n",
    "train_img3 = []\n",
    "for i in range(len_train):\n",
    "    train_img3.append(train_img2.iloc[i])\n",
    "    \n",
    "train_img4 = np.reshape(train_img3,(len_train,42,42,3))\n",
    "train_img4 = train_img4.astype('float32') / 255 # Se reescalan de 0 a 1 los valores de la imagen\n",
    "\n",
    "# Se elimina la primera dimensi√≥n de las im√°genes empleada al empaquetar la imagen durante la carga dentro del df\n",
    "test_img2 = test_img.TCI.apply(lambda item:  np.array([item[0], item[1], item[2]]))\n",
    "test_img3 = []\n",
    "for i in range(len_test):\n",
    "    test_img3.append(test_img2.iloc[i])\n",
    "\n",
    "test_img4 = np.reshape(test_img3,(len_test,42,42,3))\n",
    "test_img4 = test_img4.astype('float32') / 255 # Se reescalan de 0 a 1 los valores de la imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701702f6-9165-45df-845c-6685e436e0b1",
   "metadata": {},
   "source": [
    "Escalado features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a29cd-daf5-4293-8202-2ffb741a42fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Temperatura\n",
    "# M√°ximo y m√≠nimo de train\n",
    "maxs = np.max(train_features['T'], axis=0)\n",
    "mins = np.min(train_features['T'], axis=0)\n",
    "ranges = maxs - mins\n",
    "\n",
    "# Escalado entre 0 y 1 de la media de concentracion de cla-a. SE NORMALIZA CON LOS VALORES EXTREMOS DE TRAIN PARA NO FILTRAR INFORMACI√ìN AL MODELO.\n",
    "train_features['T'] = (train_features['T'] - mins) / ranges\n",
    "test_features['T'] = (test_features['T'] - mins) / ranges\n",
    "\n",
    "# Cla-a valor medio\n",
    "# M√°ximo y m√≠nimo de train\n",
    "maxs = np.max(train_features.cla_mean, axis=0)\n",
    "mins = np.min(train_features.cla_mean, axis=0)\n",
    "ranges = maxs - mins\n",
    "\n",
    "# Escalado entre 0 y 1 de la media de concentracion de cla-a. SE NORMALIZA CON LOS VALORES EXTREMOS DE TRAIN PARA NO FILTRAR INFORMACI√ìN AL MODELO.\n",
    "train_features.cla_mean = (train_features.cla_mean - mins) / ranges\n",
    "test_features.cla_mean = (test_features.cla_mean - mins) / ranges\n",
    "\n",
    "# Cla-a std\n",
    "# M√°ximo y m√≠nimo de train\n",
    "maxs = np.max(train_features.cla_std, axis=0)\n",
    "mins = np.min(train_features.cla_std, axis=0)\n",
    "ranges = maxs - mins\n",
    "\n",
    "# Escalado entre 0 y 1 de la media de concentracion de cla-a. SE NORMALIZA CON LOS VALORES EXTREMOS DE TRAIN PARA NO FILTRAR INFORMACI√ìN AL MODELO.\n",
    "train_features.cla_std = (train_features.cla_std - mins) / ranges\n",
    "test_features.cla_std = (test_features.cla_std - mins) / ranges\n",
    "\n",
    "# Superficie visible de agua\n",
    "# M√°ximo y m√≠nimo de train\n",
    "maxs = np.max(train_features.water_size, axis=0)\n",
    "mins = np.min(train_features.water_size, axis=0)\n",
    "ranges = maxs - mins\n",
    "\n",
    "# Escalado entre 0 y 1 de la media de concentracion de cla-a. SE NORMALIZA CON LOS VALORES EXTREMOS DE TRAIN PARA NO FILTRAR INFORMACI√ìN AL MODELO.\n",
    "train_features.water_size = (train_features.water_size - mins) / ranges\n",
    "test_features.water_size = (test_features.water_size - mins) / ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085cd5f5-9599-48ff-95f1-0d7f0495b34e",
   "metadata": {},
   "source": [
    "Construcci√≥n del vector de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674637f0-7bea-4695-b147-e481dbd7f8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features2 = np.array([train_features['cla_mean'], train_features['cla_std'], train_features['water_size'], train_features['T'], train_features[\"invierno\"], train_features['primavera'], train_features[\"verano\"], train_features[\"oto√±o\"], train_features['south'], train_features['west'], train_features[\"midwest\"], train_features[\"northeast\"]]).T\n",
    "train_features3 = train_features2.astype('float32')\n",
    "\n",
    "test_features2 = np.array([test_features['cla_mean'], test_features['cla_std'], test_features['water_size'], test_features['T'], test_features[\"invierno\"], test_features['primavera'], test_features[\"verano\"], test_features[\"oto√±o\"], test_features['south'], test_features['west'], test_features[\"midwest\"], test_features[\"northeast\"]]).T\n",
    "test_features3 = test_features2.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d572b4d0-b2bc-48ec-ae92-5e9babea661b",
   "metadata": {},
   "source": [
    "Escalado del target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fcd0b-ebed-4323-b863-e6e6219b4fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_labels = test_labels.astype('float32') / np.max(train_labels, axis=0)\n",
    "train_labels = train_labels.astype('float32') / np.max(train_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d79c98-f5b0-496e-983d-a5d53350d622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_features = len(train_features3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e9335-923c-49ba-9b75-9680c5231040",
   "metadata": {},
   "source": [
    "## Paso 3: Creacion y evaluaci√≥n de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d28d1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modelo 1: Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed816dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "\n",
    "# Aplanar los datos y agregar capas densas (fully connected)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='tanh')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_simple = models.Model(img_input, output)\n",
    "\n",
    "model_simple.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf9ad90-25c9-4501-b883-3451c1217fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'base_weights.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98901952-35dc-4f53-80e0-3427bdde75c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57463fb2-06b6-40d7-aa94-3dd47e8689d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_simple.compile(optimizer='rmsprop',\n",
    "              loss='mae',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e7bd9a-d5e7-429f-9625-0cafcad27778",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_base_model = model_simple.fit(x=train_img4, y=train_labels, epochs=50, validation_data=(test_img4, test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52693e9-d154-4ccc-a3f4-fccfd9d69881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_simple.load_weights(DATA_DIR / 'base_weights.43-0.1512.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc045a82-d40e-4b49-ad24-b17d49386cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_simple.save(\"model_simple.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8d96a-3df2-4439-ae22-5cc1b8c0e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_simple = keras.models.load_model(\"model_simple.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52a1be-8b20-4340-8abd-83fd163d7475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(50), history_base_model.history['loss'], label='train')\n",
    "ax.plot(range(50),history_base_model.history['val_loss'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MAE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f83ac-500a-49db-9fd9-dda5e5a987f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(50), history_base_model.history['mse'], label='train')\n",
    "ax.plot(range(50),history_base_model.history['val_mse'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cddea64-0769-43fa-b09f-495a9a59fbbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_simple.predict(train_img4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b291a62-62a4-4eff-ba3c-84147d3b963c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05df23-0169-42db-84f8-73a32be6ba1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d916df8-f833-43df-95c1-755940cba508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b9d8c-c53e-42af-9ebd-2dda10f9c12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_simple.predict(test_img4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6249f3-b283-4ef0-af59-236fcec01797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc72e0-1cf9-4501-8982-6fdc97d5424b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f451100-4c1f-4c16-8cdb-3dfc96a2dbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2e13e-e6d0-44e2-b26d-e5222a5d8180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_simple.evaluate(train_img4, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f516c-56e3-4f48-9e72-18b4e822fbae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_simple.evaluate(test_img4, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4ee4c-8b0f-4399-b644-b04f7da46c33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modelo 2: Convolucional + features en paralelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6916c2-ebbf-4cfb-8f90-168536fe9098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "features_input = layers.Input(shape=(number_of_features,))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "\n",
    "# Aplanar los datos y agregar capas densas (fully connected)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Se introduce el output de la √∫ltima capa convolucional como input junto con las features calculadas\n",
    "x = layers.Dense(128, activation='relu')(features_input)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.concatenate([y, x])\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_features = models.Model([img_input, features_input], output)\n",
    "\n",
    "model_features.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2a86d-0dc0-4fb6-a057-0a9e7da72934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'features_weights.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f948037-db89-48de-894d-6a46b6292241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9474d0-ad55-4738-b812-3459eea86bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features.compile(optimizer='rmsprop',\n",
    "              loss='mae',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ac170-6881-46ee-affd-db2b5192e317",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_features_model = model_features.fit(x=[train_img4, train_features3], y=train_labels, epochs=100, validation_data=([test_img4, test_features3], test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fd9cc-b556-4ee4-a097-3e3690996985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features.load_weights(DATA_DIR / 'features_weights.65-0.1253.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52a39e-8115-4d69-94c9-fa41953548c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features.save(\"model_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646e22d-629c-43d5-ba0a-9f904c3a0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_features = keras.models.load_model(\"model_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39284a00-2999-4b6e-ae68-4d00677ad2da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(100), history_features_model.history['loss'], label='train')\n",
    "ax.plot(range(100),history_features_model.history['val_loss'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0337f8-277f-4ed9-aa9d-63b0bbd05b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(100), history_features_model.history['mse'], label='train')\n",
    "ax.plot(range(100),history_features_model.history['val_mse'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2380e48-0841-4ba5-b699-e55a056a6d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features.predict([train_img4, train_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83812f7-4cbc-4f36-9a42-1cc6bd13be45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeafacc-8602-421b-acea-38b0ca75893f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c51b1-2be6-4a4b-8f43-68aee2dcb11d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d384ad-46f4-486d-969c-419c02ada863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features.predict([test_img4, test_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddbc1d-f843-4a9f-a158-3e9f866b2b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025efef-927a-450f-94e9-dcd0eb44403f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f6158-a7f3-4a5c-bc9e-0e374527ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_acc.count(1) / (binary_acc.count(1) + binary_acc.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9c060-f155-445c-928d-576a2bd82b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2c0ed-f826-46c6-beb5-ffd2cd2ee171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features.evaluate([train_img4, train_features3], train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ec9ccb-7c34-4336-9139-e593c5517443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features.evaluate([test_img4, test_features3], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7583c-9721-4252-ba7a-0350709cafec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modelo 3: Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1cc34-04a6-4881-9579-9564d7219e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x1 = layers.Conv2D(16, (1, 1), strides = 2, activation='relu')(img_input)\n",
    "x1 = layers.Cropping2D(cropping=((2, 1), (2, 1)))(x1)\n",
    "\n",
    "x2 = layers.Conv2D(32, (1, 1), strides = 1, activation='relu')(img_input)\n",
    "x2 = layers.Conv2D(16, (3, 3), strides = 2, activation='relu')(x2)\n",
    "x2 = layers.Cropping2D(cropping=((1, 1), (1, 1)))(x2)\n",
    "\n",
    "x3 = layers.AveragePooling2D((3, 3), strides = 2)(img_input)\n",
    "x3 = layers.Conv2D(16, (3, 3), strides = 1, activation='relu')(x3)\n",
    "\n",
    "x4 = layers.Conv2D(32, (1, 1), strides = 1, activation='relu')(img_input)\n",
    "x4 = layers.Conv2D(16, (3, 3), strides = 1, activation='relu')(x4)\n",
    "x4 = layers.Conv2D(16, (3, 3), strides = 2, activation='relu')(x4)\n",
    "x4 = layers.Cropping2D(cropping=((0, 1), (0, 1)))(x4)\n",
    "\n",
    "x = layers.concatenate([x1, x2, x3, x4])\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_inception = models.Model(img_input, output)\n",
    "\n",
    "model_inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e77ccd-af5d-4725-a9ad-e0d1f13589b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'inception_weights.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee99cf2-2101-4b92-b65e-c796453c0644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d9dc2-db41-4244-8c37-4d2b138814d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_inception.compile(optimizer='rmsprop',\n",
    "              loss='mae',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345a55b-5bd8-4c27-86b4-f44ac1058461",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_inception_model = model_inception.fit(x=train_img4, y=train_labels, epochs=50, validation_data=(test_img4, test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d099d-394a-4985-b4cb-d48a94292f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_inception.load_weights(DATA_DIR / 'inception_weights.09-0.1353.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a8a2e3-8efe-4cc0-a4f8-f79130bd0c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_inception.save(\"model_inception.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a8b3c-c8f0-41bc-94b5-27ce57cb71d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inception = keras.models.load_model(\"model_inception.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4732a-a278-48e2-bcbd-de4f689555d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(50), history_inception_model.history['loss'], label='train')\n",
    "ax.plot(range(50),history_inception_model.history['val_loss'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MAE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ea885-a917-433f-a9e6-1280858ae2be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(50), history_inception_model.history['mse'], label='train')\n",
    "ax.plot(range(50),history_inception_model.history['val_mse'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b4ef0-7eac-422c-bc35-49593d5cdf16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_inception.predict(train_img4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3d091-f278-42a7-8a4b-099b6d21fe0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b4ac5-83bf-4cd1-93d0-b8af5222f4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315ad24-61d2-4ddd-a5a6-93f0d78cd416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211593a0-36dd-4739-9373-2f23d8eceb38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_inception.predict(test_img4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3bdeb-5a6e-42a7-b5f7-e5540c112468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03508df-a8b4-4281-9c7c-00d790d06d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff50227-b575-40b2-90ae-4f625e04b8d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf62b6-e433-4c3f-b44f-14464b8c3c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_inception.evaluate(test_img4, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259cdb49-c55b-4091-97f2-8bba61d4846c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_inception.evaluate(train_img4, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceeaa8d-5088-4e2d-81d9-a7f50b57da13",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modelo 4: Inception + features en paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f70ca7-1456-4b07-8f87-2bb4e0c6bb02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "features_input = layers.Input(shape=(number_of_features,))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x1 = layers.Conv2D(16, (1, 1), strides = 2, activation='relu')(img_input)\n",
    "x1 = layers.Cropping2D(cropping=((2, 1), (2, 1)))(x1)\n",
    "\n",
    "x2 = layers.Conv2D(32, (1, 1), strides = 1, activation='relu')(img_input)\n",
    "x2 = layers.Conv2D(16, (3, 3), strides = 2, activation='relu')(x2)\n",
    "x2 = layers.Cropping2D(cropping=((1, 1), (1, 1)))(x2)\n",
    "\n",
    "x3 = layers.AveragePooling2D((3, 3), strides = 2)(img_input)\n",
    "x3 = layers.Conv2D(16, (3, 3), strides = 1, activation='relu')(x3)\n",
    "\n",
    "x4 = layers.Conv2D(32, (1, 1), strides = 1, activation='relu')(img_input)\n",
    "x4 = layers.Conv2D(16, (3, 3), strides = 1, activation='relu')(x4)\n",
    "x4 = layers.Conv2D(16, (3, 3), strides = 2, activation='relu')(x4)\n",
    "x4 = layers.Cropping2D(cropping=((0, 1), (0, 1)))(x4)\n",
    "\n",
    "x = layers.concatenate([x1, x2, x3, x4])\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "y = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "# Se introduce el output de la √∫ltima capa convolucional como input junto con las features calculadas\n",
    "x = layers.Dense(64, activation='relu')(features_input)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.concatenate([y, x])\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_inception_features = models.Model([img_input,features_input], output)\n",
    "\n",
    "model_inception_features.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00907574-1036-4ce0-90c7-63d4cb7042e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'inception_features_weights.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cba666-d3fe-468d-8301-43c5786be2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443b59a-2875-4e70-a71d-896a3fbb81c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_inception_features.compile(optimizer='rmsprop',\n",
    "              loss='mae',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c4a21-85fd-43f4-9e26-a9dab718cb23",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_inception_features_model = model_inception_features.fit(x=[train_img4, train_features3], y=train_labels, epochs=50, validation_data=([test_img4, test_features3], test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc9329-17d7-455f-992e-8330526284ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_inception_features.load_weights(DATA_DIR / 'inception_features_weights.10-0.0334.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971bb69c-414b-4d3c-a69f-aac3a65a95ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_inception_features.save(\"model_inception_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc87dc-41d0-44ea-bdfc-ca7b44abc5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inception_features = keras.models.load_model(\"model_inception_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce04bc-f55b-4741-94b6-6df5716141b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(50), history_inception_features_model.history['loss'], label='train')\n",
    "ax.plot(range(50),history_inception_features_model.history['val_loss'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MAE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59abaa-579c-4172-96de-039c8f2f590c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(50), history_inception_features_model.history['mse'], label='train')\n",
    "ax.plot(range(50),history_inception_features_model.history['val_mse'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c479e-8601-4bcf-bfa6-01587672defe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_inception_features.predict([train_img4, train_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e64a77-442e-4e48-93f5-91f361a3854d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4403a8a-3d99-4fbe-af86-5b22cc1d8126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c830f8-51ea-4d5d-8952-e0ded9c86f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cc293-9925-4285-bf56-63265272d829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_inception_features.predict([test_img4, test_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4efc60-0757-4a56-b73d-a719171e8793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b6d852-5aa2-486f-bf11-50d9c1755efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e46ff-3105-4113-ad97-4a3fd62b6553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee371051-0b9a-493f-abe4-ec2b13441595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_inception_features.evaluate([train_img4, train_features3], train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d4e1f-d3e2-4a52-b938-7f5547b1285b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_inception_features.evaluate([test_img4, test_features3], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2022b988-ab10-450a-bdb7-5bfb6379951b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Paso 4: Optimizacion del modelo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016d183-236b-4a31-b1a3-129378c48e36",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reducci√≥n. 2 capa conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99707ed4-ca9e-4ba6-aa69-4e2319e0b103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "features_input = layers.Input(shape=(number_of_features,))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "\n",
    "\n",
    "# Aplanar los datos y agregar capas densas (fully connected)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Se introduce el output de la √∫ltima capa convolucional como input junto con las features calculadas\n",
    "x = layers.Dense(128, activation='relu')(features_input)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.concatenate([y, x])\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_features_red_2lay = models.Model([img_input, features_input], output)\n",
    "\n",
    "model_features_red_2lay.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c1599-55b5-4601-9c29-664a5ca09787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'model_features_red_2lay.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf617e2-bd1d-4f6b-a8d6-bfc44c2dc46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eec3a8-6c34-4a88-8d44-3b462b8b86fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_red_2lay.compile(optimizer='rmsprop',\n",
    "              loss='mae',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73862a4-1cf5-40ea-be89-6d7835f90078",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_model_features_red_2lay = model_features_red_2lay.fit(x=[train_img4, train_features3], y=train_labels, epochs=50, validation_data=([test_img4, test_features3], test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7faa03-5913-4cf0-8bdc-608f6d0a58a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_red_2lay.load_weights(DATA_DIR / 'model_features_red_2lay.49-0.1249.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c2637-41ce-4f19-9498-2bd9c158652a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_red_2lay.save(\"model_features_red_2lay.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667865c9-4849-4b01-96bf-0117e2554df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_features = keras.models.load_model(\"model_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b9a18-ede1-47b2-9fe4-ac240165cb0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(50), history_model_features_red_2lay.history['loss'], label='train')\n",
    "ax.plot(range(50),history_model_features_red_2lay.history['val_loss'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MAE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede68a8-9d6f-445a-9489-7b51d201d94b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(50), history_model_features_red_2lay.history['mse'], label='train')\n",
    "ax.plot(range(50),history_model_features_red_2lay.history['val_mse'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235934b3-80fe-4501-bfe5-9e0dc2d86fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_red_2lay.predict([train_img4, train_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d6045-d495-45bf-91e8-8f4fe6277b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b6c94-9dde-4948-b0b6-09a86f3e075b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65a7ef-755c-4bdf-8e54-d7577a5f50f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0197d3-7946-4146-8825-377171d44f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_red_2lay.predict([test_img4, test_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc5d21-a48e-4571-9883-a648b49abe62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe78d5-f7da-490b-ae74-cddefebf18df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aa9a70-5d10-4b58-bf06-20b85b5e4845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fc383-5bad-4d7b-9b5b-4cb44d156b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_red_2lay.evaluate([test_img4, test_features3], test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b50b2c-767d-4e7a-8925-7e17ca5d1d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_red_2lay.evaluate([train_img4, train_features3], train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4e8fd-e569-4179-8e34-4f3b3cd8768c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reducci√≥n. 1 capa conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67230265-ec66-4e2a-bb6a-7771c7a9980f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "features_input = layers.Input(shape=(number_of_features,))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(img_input)\n",
    "\n",
    "# Aplanar los datos y agregar capas densas (fully connected)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Se introduce el output de la √∫ltima capa convolucional como input junto con las features calculadas\n",
    "x = layers.Dense(128, activation='relu')(features_input)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.concatenate([y, x])\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_features_red_1lay = models.Model([img_input, features_input], output)\n",
    "\n",
    "model_features_red_1lay.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d9d55-2844-48cc-a508-8042af1b2216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'model_features_red_1lay.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ad0ca-99c0-4eb7-ab98-7ea40cb3002c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3bcc8-67cb-4572-ab29-3747a288bb0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_red_1lay.compile(optimizer='rmsprop',\n",
    "              loss='mae',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbc53f-fa4e-4617-8580-cd6128cac9e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_model_features_red_1lay = model_features_red_1lay.fit(x=[train_img4, train_features3], y=train_labels, epochs=50, validation_data=([test_img4, test_features3], test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f95268-63fb-4659-8743-75f8c3787e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_red_1lay.load_weights(DATA_DIR / 'model_features_red_1lay.47-0.1268.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab86b33-9c3a-4cfa-b966-7935dc821d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_red_1lay.save(\"model_features_red_1lay.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3408e7-8cd1-4c40-9bc8-f85e08bf23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_features = keras.models.load_model(\"model_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906e0a0-5b81-4146-b011-a216d02db01f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(range(50), history_model_features_red_1lay.history['loss'],range(50),history_model_features_red_1lay.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b665a-fc90-45bb-91b7-5c57e828e71c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_red_1lay.predict([train_img4, train_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561189e-5e44-4a39-b3d4-3b02d1dbb26c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f4fca-f961-439a-9b7b-ef3a68b3039d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d5d0e-e635-4405-ad5d-63465f32e237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd77ef-5270-4dcc-aa8e-3c77a3f431fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_red_1lay.predict([test_img4, test_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e089a-098a-45fa-8371-11bd060a8d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7181c-e906-4ec3-9bb8-8dd6583c1f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630c767-ed51-401b-aa58-42f1fb1979e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a6fec-6154-405c-b96e-f7ed1c960268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_red_1lay.evaluate([train_img4, train_features3], train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652bbd4c-2514-4729-bf3c-60168460807d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_red_1lay.evaluate([test_img4, test_features3], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad17607-1679-490a-b664-6634a231770a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reducci√≥n. 4 capa conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22871b33-6bb8-4626-b0fd-6b4cf5f18634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "features_input = layers.Input(shape=(number_of_features,))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "\n",
    "# Aplanar los datos y agregar capas densas (fully connected)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Se introduce el output de la √∫ltima capa convolucional como input junto con las features calculadas\n",
    "x = layers.Dense(128, activation='relu')(features_input)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.concatenate([y, x])\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_features_red_4lay = models.Model([img_input, features_input], output)\n",
    "\n",
    "model_features_red_4lay.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e41d92-4de5-4dc3-8422-60b9b9681c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'model_features_red_4lay.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9c23f-8738-45c5-9466-9bc377d9722b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72807b74-617d-41a4-b8e4-54c1316043a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_red_4lay.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['mae', tf.keras.metrics.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a33d7-490f-4e0f-b987-3318b21cd2f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_model_features_red_4lay = model_features_red_4lay.fit(x=[train_img4, train_features3], y=train_labels, epochs=50, validation_data=([test_img4, test_features3], test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1bd837-90e0-4fef-b535-08da5cae2773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_red_4lay.load_weights(DATA_DIR / 'model_features_red_4lay.15-0.0342.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f3489-c7b1-4ae7-83fa-1bb678083997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_red_4lay.save(\"model_features_red_4lay.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627bb753-62f7-4d98-b12b-4c64953546f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_features_red_4lay = keras.models.load_model(\"model_features_red_4lay.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242e1be-b70c-43b5-9bcb-98afaa425a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(range(50), history_model_features_red_4lay.history['loss'],range(50),history_model_features_red_4lay.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f10135-7293-4498-929d-6b441e888129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_red_4lay.predict([train_img4, train_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bdd031-e62f-483c-a45c-8257bc3e8d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfa036-5e1d-41a3-b481-356776756163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074b1ed-9d89-4d0f-9d3d-0890ad427254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f930856-bdd6-47e6-ab60-17517ed9203a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_red_4lay.predict([test_img4, test_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a7057-8ba3-4afe-8607-449aa27f98a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c1df9-4adf-463b-8e3b-8ff986b8f461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd1720-c8a9-48ed-b4e1-f2337ea1b06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1bd9a-ca4c-4684-9432-43f97575d9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_red_4lay.evaluate([train_img4, train_features3], train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94441465-7771-4b5c-8da6-1a7b787e5ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_red_4lay.evaluate([test_img4, test_features3], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c33f4-f704-4560-ae27-6b5e25960484",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neuronas. x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a657c-1f09-4015-9ed5-b4cbb9347708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "features_input = layers.Input(shape=(number_of_features,))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x = layers.Conv2D(32*2, (3, 3), activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64*2, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64*2, (3, 3), activation='relu')(x)\n",
    "\n",
    "# Aplanar los datos y agregar capas densas (fully connected)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128*2, activation='relu')(x)\n",
    "x = layers.Dense(64*2, activation='relu')(x)\n",
    "y = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Se introduce el output de la √∫ltima capa convolucional como input junto con las features calculadas\n",
    "x = layers.Dense(128*2, activation='relu')(features_input)\n",
    "x = layers.Dense(64*2, activation='relu')(x)\n",
    "x = layers.concatenate([y, x])\n",
    "x = layers.Dense(64*2, activation='relu')(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_features_neur2 = models.Model([img_input, features_input], output)\n",
    "\n",
    "model_features_neur2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a590879-8cc6-461b-bad8-06f5fa0d5ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'model_features_neur2.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc4030-8008-4d3a-83d2-670ff9a74d94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fad689-03b3-41d9-aae5-0fef2090b5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_neur2.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['mae', tf.keras.metrics.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0b35c-07a3-4e3a-a7ba-79e4d7236104",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_model_features_neur2 = model_features_neur2.fit(x=[train_img4, train_features3], y=train_labels, epochs=50, validation_data=([test_img4, test_features3], test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddb26c-957d-40cc-ba3d-4287319e4bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_neur2.load_weights(DATA_DIR / 'model_features_neur2.15-0.0327.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92f7b5-e400-46ee-9de9-2022853e3875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_neur2.save(\"model_features_neur2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0c670-7cbe-4b58-8a6a-b3bce9c87a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_features_neur2 = keras.models.load_model(\"model_features_neur2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a39b6b-0913-4c0d-9d9c-6e0526c9aed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(range(50), history_model_features_neur2.history['loss'],range(50),history_model_features_neur2.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753ad21-f28d-4882-bb5f-7a5d0fc7e7fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_neur2.predict([train_img4, train_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f005c-86f3-463b-85ec-82d37ab9bf30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb42b2-22f8-4f2c-8396-76c723650921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a16ba-1506-427a-87bd-05f3db1bfff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb862391-9100-49c7-97e9-9863aa8bff3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_neur2.predict([test_img4, test_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e272c0-2634-45d1-9f9b-46ef242f2b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff176d4-324e-43cf-9586-04cd5c9e7664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c8778-a096-42c6-a195-7cdcac1def93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b38d9-f9ee-4b21-98a2-bed2ba0367c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_neur2.evaluate([train_img4, train_features3], train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d4d62-d140-4797-a730-520346672b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_neur2.evaluate([test_img4, test_features3], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b1f7f-d3a9-42cd-ba6d-04f075995a4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neuronas. x 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511627d-a8c6-485c-b5a9-d3bedd170d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "features_input = layers.Input(shape=(number_of_features,))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x = layers.Conv2D(32/2, (3, 3), activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64/2, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64/2, (3, 3), activation='relu')(x)\n",
    "\n",
    "# Aplanar los datos y agregar capas densas (fully connected)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128/2, activation='relu')(x)\n",
    "x = layers.Dense(64/2, activation='relu')(x)\n",
    "y = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Se introduce el output de la √∫ltima capa convolucional como input junto con las features calculadas\n",
    "x = layers.Dense(128/2, activation='relu')(features_input)\n",
    "x = layers.Dense(64/2, activation='relu')(x)\n",
    "x = layers.concatenate([y, x])\n",
    "x = layers.Dense(64/2, activation='relu')(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_features_neur05 = models.Model([img_input, features_input], output)\n",
    "\n",
    "model_features_neur05.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52147273-5e88-468f-bb69-6be4e204b774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'model_features_neur05.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddab6ba-6b63-4f08-b725-0bdd50da4e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13263e7-2ad0-42ce-9e0b-7d6b3dd3c257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_neur05.compile(optimizer='rmsprop',\n",
    "              loss='mae',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efb5a9-7265-49d2-a101-81ea3c531270",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_model_features_neur05 = model_features_neur05.fit(x=[train_img4, train_features3], y=train_labels, epochs=50, validation_data=([test_img4, test_features3], test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d90bba-7e4c-4d09-8360-2f5e765e5ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_neur05.load_weights(DATA_DIR / 'model_features_neur05.48-0.1297.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a1163-0290-4b54-a529-f17894894f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_neur05.save(\"model_features_neur05.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546ff8d-b010-4a5c-be7d-275b45f69b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_features_neur2 = keras.models.load_model(\"model_features_neur2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d79607-fc31-47fc-95c0-75a1196a2308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(range(50), history_model_features_neur05.history['loss'],range(50),history_model_features_neur05.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7be3f-e7c5-4c98-b439-a166f0b41753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_neur05.predict([train_img4, train_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6329445-1744-4398-9b0c-3b4ab59bff70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7388ea-acbe-455d-8a82-95a44fdc8c82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0855a-83da-47c1-9f5f-f07f32c59bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffe811-ed32-4a91-ab79-b0e28b2a41d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_neur05.predict([test_img4, test_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90dba1-2840-4c54-8118-f45ba4839cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556cc2bb-5577-41c3-bed5-95cffa2de3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d662038-2ab9-4661-8d40-1d3985d5d30a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9a611-e6f1-4e42-a95f-70a927c456fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_neur05.evaluate([train_img4, train_features3], train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de608225-e485-4dc1-9cdb-9962af39a2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_neur05.evaluate([test_img4, test_features3], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90417ec1-3a25-4ac6-9ea2-ed6372e62df1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neuronas. x 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06e7bc-cd5f-4927-87cc-2168b476d992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "features_input = layers.Input(shape=(number_of_features,))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x = layers.Conv2D(32*4, (3, 3), activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64*4, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64*4, (3, 3), activation='relu')(x)\n",
    "\n",
    "# Aplanar los datos y agregar capas densas (fully connected)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128*4, activation='relu')(x)\n",
    "x = layers.Dense(64*4, activation='relu')(x)\n",
    "y = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Se introduce el output de la √∫ltima capa convolucional como input junto con las features calculadas\n",
    "x = layers.Dense(128*4, activation='relu')(features_input)\n",
    "x = layers.Dense(64*4, activation='relu')(x)\n",
    "x = layers.concatenate([y, x])\n",
    "x = layers.Dense(64*4, activation='relu')(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_features_neur4 = models.Model([img_input, features_input], output)\n",
    "\n",
    "model_features_neur4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06b304-e123-4f74-bb3c-e96e3b313ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'model_features_neur4.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798e3a0-dafa-4a0c-8256-36b599b4b7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c6066-14d9-4284-b2ee-e822c0ccb6b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_neur4.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['mae', tf.keras.metrics.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad456820-d73b-4e74-b8cd-d8de8f830496",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_model_features_neur4 = model_features_neur4.fit(x=[train_img4, train_features3], y=train_labels, epochs=50, validation_data=([test_img4, test_features3], test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0b14e-a8dd-4712-87f4-d333bc565dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_neur4.load_weights(DATA_DIR / 'model_features_neur4.12-0.0334.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e249b-0b7a-447a-a1ff-dc937ddbbd7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features_neur4.save(\"model_features_neur4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727aa09-1c69-4fdc-89cc-0cc33aab26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_features_neur4 = keras.models.load_model(\"model_features_neur4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e637e869-f11d-4973-8762-11ca6f98266c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(range(50), history_model_features_neur4.history['loss'],range(50),history_model_features_neur4.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f015b28-61b7-424a-8f9b-cbb8a4b66696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_neur4.predict([train_img4, train_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab0323-9bc8-41e0-991c-cb220641aa50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4a1ef-11b6-4069-b12d-657de264ef63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2256ffc-45b4-414b-a826-af31c187d934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb451e8-ef75-412f-9fb4-4dab57fd71a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_features_neur4.predict([test_img4, test_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b22083b-56c7-47b2-adc0-d72e4a706473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070476e-529c-4942-9959-10a58975c47d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5c70c-6717-41e8-b0c9-4bf23a47048a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17be96e-8e3c-4545-85ac-fe72fcd1b517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_neur4.evaluate([train_img4, train_features3], train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3215e-d0db-4eab-9c7b-4c7dfb708d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_features_neur4.evaluate([test_img4, test_features3], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba7fe9-59a5-4803-bfa5-52464239c7eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### QI > 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212737e-14ec-4a59-b199-d3f61b2c689a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "features_input = layers.Input(shape=(number_of_features,))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "\n",
    "# Aplanar los datos y agregar capas densas (fully connected)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Se introduce el output de la √∫ltima capa convolucional como input junto con las features calculadas\n",
    "x = layers.Dense(128, activation='relu')(features_input)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.concatenate([y, x])\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_featuresqi25 = models.Model([img_input, features_input], output)\n",
    "\n",
    "model_featuresqi25.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c2963-d824-4660-8c70-bea9ff06f3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'features_weightsqi25.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02a0b7-d168-45e3-97c6-720d155d0513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441699b5-3b50-42bf-bdae-ac8ae71ced06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_featuresqi25.compile(optimizer='rmsprop',\n",
    "              loss='mae',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6298a9d-0caa-42bc-94ad-94f8152814a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_featuresqi25_model = model_featuresqi25.fit(x=[train_img4, train_features3], y=train_labels, epochs=100, validation_data=([test_img4, test_features3], test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3719ff5-61b6-4f2a-8f73-3c8cbdf3b10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_featuresqi25.load_weights(DATA_DIR / 'features_weightsqi25.90-0.1219.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e149d-8b0a-4fbe-ab4e-0e2c5aa2cb60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_featuresqi25.save(\"model_featuresqi25.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9121c69-2c7c-4b5b-9425-0c76e49b0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_features = keras.models.load_model(\"model_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744a792-9333-40f6-9f86-f07f04031a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(100), history_featuresqi25_model.history['loss'], label='train')\n",
    "ax.plot(range(100),history_featuresqi25_model.history['val_loss'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a3cbdd-6522-4940-8cd8-f0c2149db939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(100), history_featuresqi25_model.history['mse'], label='train')\n",
    "ax.plot(range(100),history_featuresqi25_model.history['val_mse'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced78af7-99f0-42b2-9cf6-89f559bb0fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_featuresqi25.predict([train_img4, train_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa55812-b346-48bb-b57d-bba8fcce0223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c9db6-e623-45b1-8d5f-64781faa9655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0373e-4674-4364-9235-3769ec13a550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16a9c0-cb91-493b-88d6-3ef49a2dfd8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_featuresqi25.predict([test_img4, test_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dcd165-33c7-494a-9b24-f603c6c2ee4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468dff9-ae47-403d-9603-c477d24ca6be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8182e-8234-4795-90e9-13242d1a9a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619734d0-3641-4dc6-ba24-881c04ca1e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_featuresqi25.evaluate([train_img4, train_features3], train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553632d6-8e2a-4ce7-b807-62b2948e50b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_featuresqi25.evaluate([test_img4, test_features3], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f9f6e-7eaf-442a-ab34-2b2595111a59",
   "metadata": {
    "tags": []
   },
   "source": [
    "### QI > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956b919-657d-4734-aa18-9d1936358312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregar capas de entrada\n",
    "img_input = layers.Input(shape=(SHAPE[1], SHAPE[2], SHAPE[0]))\n",
    "features_input = layers.Input(shape=(number_of_features,))\n",
    "\n",
    "# Agregar capas convolucionales. Procesado imagen\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "\n",
    "# Aplanar los datos y agregar capas densas (fully connected)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Se introduce el output de la √∫ltima capa convolucional como input junto con las features calculadas\n",
    "x = layers.Dense(128, activation='relu')(features_input)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.concatenate([y, x])\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_featuresqi50 = models.Model([img_input, features_input], output)\n",
    "\n",
    "model_featuresqi50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df57f4-6fca-414f-9d6c-0b0fe7c9bcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = DATA_DIR / 'features_weightsqi50.{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b8284-f430-4222-9fd6-a802af4f6853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4e8ab-b4f0-4b8f-994e-b5561fc70a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_featuresqi50.compile(optimizer='rmsprop',\n",
    "              loss='mae',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dab0c2-4087-4c95-918c-7936becb6d15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_featuresqi50_model = model_featuresqi50.fit(x=[train_img4, train_features3], y=train_labels, epochs=15, validation_data=([test_img4, test_features3], test_labels), callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d444c78-6f4f-4f71-9d07-912fc84b5af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_featuresqi50.load_weights(DATA_DIR / 'features_weightsqi50.44-0.1186.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca45899-0236-421b-afbd-b69c9967006e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_featuresqi50.save(\"model_featuresqi25.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608c105-2f8d-4669-b7fd-4e33ddfe30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_features = keras.models.load_model(\"model_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91bd3c8-8a9c-4bee-ad06-193cd28981ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(100), history_featuresqi50_model.history['loss'], label='train')\n",
    "ax.plot(range(100),history_featuresqi50_model.history['val_loss'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592439c-5564-45b9-98b7-a659f57752b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flg,ax = plt.subplots()\n",
    "ax.plot(range(100), history_featuresqi50_model.history['mse'], label='train')\n",
    "ax.plot(range(100),history_featuresqi50_model.history['val_mse'], label='test')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9f51e-f77f-4665-a344-0f5a5da87b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_featuresqi50.predict([train_img4, train_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd43aab9-04ce-4526-97af-f211b5241e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, train_labels, train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7064164-f023-4c13-888e-f38ffda7958d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af058d51-bd9d-495c-af69-098e3e918003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30eb9f8-466b-4233-9eda-27cc8cda76e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_featuresqi50.predict([test_img4, test_features3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecaeda-8363-4dea-9bd2-dc85f5ab0d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rounded_result, acc, rmse, rmae = reconversion_target(predictions, test_labels, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80919fde-75c6-4787-bb60-ca5854988d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, rmse, rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed5b1d-c5f5-497e-b2cd-d6856a69f4b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_percent_error(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6922403-19b2-4bec-af15-b3b59a2fb6de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_featuresqi50.evaluate([train_img4, train_features3], train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8431307-aa68-4b43-82a9-bc4d17763eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluations = model_featuresqi50.evaluate([test_img4, test_features3], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaebbe9-e77f-4f08-a0d8-005b11d4018c",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220db51-bedc-428c-8d03-9ca1c62bcffb",
   "metadata": {},
   "source": [
    "[1] Keras. API docs: https://keras.io/api/\n",
    "\n",
    "[2] Fran√ßois Chollet. Deep Learning with Python. Manning Publications Co. Shelter Island, NY 11964. ISBN 9781617294433"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
